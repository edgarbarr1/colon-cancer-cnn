{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colon cancer image",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edgarbarr1/colon-cancer-cnn/blob/main/colon_cancer_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b304e1a"
      },
      "source": [
        "# Colon Cancer #\n",
        "### _Predicting the outcomes of colon cells to predict cancer_ ###\n",
        "\n",
        "### Business Understanding ###\n",
        "Colon cancer has been deemed the number 3 most common cancer in the world, according to the World Cancer Research Fund. Based on this statistic, it is not a surprise to know that more approximately 19 million colonoscopies are perfeormed each year in the United States.\n",
        "\n",
        "Some experts believe that some of the main causes of this cancer is the Western food diet along with living a sedentary lifestyle as well as being obese. Unfortunately, according to the CDC, the US appears to be on an upward trend in obesity which in turn increses the likelihood of men and women to develop colorectal cancers.\n",
        "\n",
        "Although the morttality rate for the most part appears to be relatively low (80% survival rate), it is important to note that like everything, there is always something to improve with either accurate test results, the time it takes to report those results and the resources available to compile said results.\n",
        "\n",
        "Currently, as per the American Cancer Society, it takes 2-3 days to report the findings of a colonoscopy biopsy.\n",
        "\n",
        "Objective\n",
        "This notebook has the objective of finding out the population that is deeply affected by colon cancer and build a Convolutional Neural Network that can get close to the 1-2% accuracy that current tests. We will also strive to have an efficient model that can give accurate results faster than 2-3 days and ideally within the time frame of \"same-day\" results.\n",
        "\n",
        "Before doing so, we will look at some mortality rates among different populations and determine whether the economic status of a population affects the mortality rate of colon cancer.\n",
        "\n"
      ],
      "id": "1b304e1a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9116373"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import pathlib\n",
        "# Packages to import and preprocess images\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Packages for our models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D, LeakyReLU\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline"
      ],
      "id": "f9116373",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIjBG7g1iuDj",
        "outputId": "e7f2130a-3750-4df7-a2b8-de1fcf1eb5e4"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "id": "aIjBG7g1iuDj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbd1156b",
        "outputId": "59375b19-64ab-4049-b873-a1a6c0021379"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "id": "bbd1156b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81a4495"
      },
      "source": [
        "Deleted all folders from zenodo except NORM which is normal and TUM which is the cancer cells"
      ],
      "id": "c81a4495"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e778f70c"
      },
      "source": [
        "Inspiration for the function in the creation of the [directories](https://www.youtube.com/watch?v=_L2uYfVV48I)"
      ],
      "id": "e778f70c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b87c4b"
      },
      "source": [
        "In order to have class balance in the dataset we will be using a total of 24,000 images for our model training, 1,800 items for the validation, and 1,720 images for our test dataset to generate predictions. This brings our total of images used to 27,520 images used in this Convolutional Neural Network."
      ],
      "id": "f3b87c4b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21OZVddtFmmZ"
      },
      "source": [
        "# PIL.Image.open('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM/NORM-AAAKGLVQ.tif')"
      ],
      "id": "21OZVddtFmmZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibm97lLLlaME"
      },
      "source": [
        "# os.listdir()"
      ],
      "id": "Ibm97lLLlaME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyLRIKjumsaO"
      },
      "source": [
        "# normal_image_count = len(list(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM/*.tif')))\n",
        "# cancer_image_count = len(list(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/TUM/*.tif')))\n",
        "# print('Normal images: {}'.format(normal_image_count))\n",
        "# print('Cancer images: {}'.format(cancer_image_count))"
      ],
      "id": "jyLRIKjumsaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syF6mGXZkQFy"
      },
      "source": [
        "The images in the dataset are in TIF format. Let's convert the images into jpegs."
      ],
      "id": "syF6mGXZkQFy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjZTTwbskGaq"
      },
      "source": [
        "z2ell to convert the TIF images to jpeg\n",
        "\n",
        "# paths = ['/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM',\n",
        "#          '/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/TUM']\n",
        "# for path in paths:\n",
        "#   for root, dirs, files in os.walk(path, topdown=False,):\n",
        "#       for name in files:\n",
        "#           print(os.path.join(root, name))\n",
        "#           #if os.path.splitext(os.path.join(root, name))[1].lower() == \".tiff\":\n",
        "#           if os.path.splitext(os.path.join(root, name))[1].lower() == \".tif\":\n",
        "#               if os.path.isfile(os.path.splitext(os.path.join(root, name))[0] + \".jpg\"):\n",
        "#                   print (\"A jpeg file already exists for %s\" % name)\n",
        "#               # If a jpeg with the name does *NOT* exist, convert one from the tif.\n",
        "#               else:\n",
        "#                   outputfile = os.path.splitext(os.path.join(root, name))[0] + \".jpg\"\n",
        "#                   try:\n",
        "#                       im = PIL.Image.open(os.path.join(root, name))\n",
        "#                       print (\"Converting jpeg for %s\" % name)\n",
        "#                       im.thumbnail(im.size)\n",
        "#                       im.save(outputfile, \"JPEG\", quality=100)\n",
        "#                   except Exception as e: \n",
        "#                     print(e)"
      ],
      "id": "ZjZTTwbskGaq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtVCWaN3Lknz"
      },
      "source": [
        "# Jpegs have been converted. Now separate into subdirectories."
      ],
      "id": "dtVCWaN3Lknz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArRWuyW8ZVSk"
      },
      "source": [
        "Now let's divide the images into subdirecotiries."
      ],
      "id": "ArRWuyW8ZVSk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsbi5eR-I3Z4"
      },
      "source": [
        "# os.chdir('drive/MyDrive/colon_dataset')"
      ],
      "id": "wsbi5eR-I3Z4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PerdFhSOI85c"
      },
      "source": [
        "# os.listdir()"
      ],
      "id": "PerdFhSOI85c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "317f608b"
      },
      "source": [
        "# Makes the subdirecoteries for the training, validation, and testing data.\n",
        "# if os.path.isdir('train/normal') is False:\n",
        "#     os.makedirs('train/normal')\n",
        "#     os.makedirs('train/cancer')\n",
        "#     os.makedirs('validation/normal')\n",
        "#     os.makedirs('validation/cancer')\n",
        "#     os.makedirs('test/')"
      ],
      "id": "317f608b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mptET7qZulj"
      },
      "source": [
        "Currently we have Tif and jpeg duplicates in our directory. Now we will move the jpeg images to the newly created directories."
      ],
      "id": "4mptET7qZulj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIeZcNn6KQuv"
      },
      "source": [
        "# Run this code to move the images from the original dataset directory to the newly created dataset that the model will read.\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM/*.jpg'), 8000):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/train/normal/')\n",
        "\n",
        "\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/TUM/*.jpg'), 8000):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/train/cancer/')\n",
        "\n",
        "\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM/*.jpg'), 400):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/validation/normal/')\n",
        "\n",
        "\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/TUM/*.jpg'), 400):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/validation/cancer/')\n",
        "\n",
        "\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/NORM/*.jpg'), 360):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/test/')\n",
        "\n",
        "\n",
        "# for image in random.sample(glob.glob('/content/drive/MyDrive/colon_dataset/NCT-CRC-HE-100K/TUM/*.jpg'), 360):\n",
        "#   shutil.move(image, '/content/drive/MyDrive/colon_dataset/test/')"
      ],
      "id": "jIeZcNn6KQuv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fawpQmtgsm4o"
      },
      "source": [
        "# current_path_list = ['/content/drive/MyDrive/colon_dataset/validation',\n",
        "#                      '/content/drive/MyDrive/colon_dataset/test',\n",
        "#                      '/content/drive/MyDrive/colon_dataset/train']"
      ],
      "id": "fawpQmtgsm4o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtNqYMuWeIMt"
      },
      "source": [
        "# Image Data Generator #"
      ],
      "id": "rtNqYMuWeIMt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrjYntT5wlRo"
      },
      "source": [
        ""
      ],
      "id": "hrjYntT5wlRo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmxTqUstUJTy"
      },
      "source": [
        "data_gen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    zoom_range = (0.95,0.95),\n",
        "    brightness_range = [0.5, 1.0]\n",
        ")"
      ],
      "id": "EmxTqUstUJTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL7yGbVjUJWR",
        "outputId": "78eacdf0-68e6-44cc-d06c-c7654f5f439b"
      },
      "source": [
        "train_generator = data_gen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/colon_dataset/train',\n",
        "    target_size = (224,224),\n",
        "    batch_size = 20,\n",
        "    color_mode = 'rgb',\n",
        "    shuffle = True,\n",
        "    class_mode = 'binary',\n",
        "    subset = 'training',\n",
        "    seed = 20\n",
        ")\n",
        "validation_generator = data_gen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/colon_dataset/validation',\n",
        "    target_size = (224,224),\n",
        "    batch_size = 20,\n",
        "    color_mode = 'rgb',\n",
        "    shuffle = True,\n",
        "    class_mode = 'binary',\n",
        "    subset = 'training',\n",
        "    seed = 20\n",
        ")"
      ],
      "id": "rL7yGbVjUJWR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16000 images belonging to 2 classes.\n",
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6eb164b"
      },
      "source": [
        "# Model 1 #"
      ],
      "id": "b6eb164b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd3FpAu7C-TC"
      },
      "source": [
        ""
      ],
      "id": "vd3FpAu7C-TC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycD3pq5C-Vb"
      },
      "source": [
        "Let's create our first model based on the data generator that we created above.\n",
        "\n",
        "Our first model will be a rather simple one with the following layers:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   A convolutional layer with a (10,10) kernel size or the height and width of our convolutional window.\n",
        "2.   A Max Pooling Layer that with a height and width of (5,5).\n",
        "1.   A Dense Layer with a 64 output size\n",
        "2.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "-ycD3pq5C-Vb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a83a3761"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5,5), activation='relu' , input_shape = (224,224,3)))\n",
        "model.add(MaxPooling2D(3,3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "id": "a83a3761",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b024ebc",
        "outputId": "afe8a0e5-3ea3-409b-a95f-aee24983b512"
      },
      "source": [
        "model.summary()"
      ],
      "id": "8b024ebc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 220, 220, 32)      2432      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 73, 73, 64)        2112      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 341056)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 341057    \n",
            "=================================================================\n",
            "Total params: 345,601\n",
            "Trainable params: 345,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apgO3Djblo3T"
      },
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import seaborn as sns"
      ],
      "id": "apgO3Djblo3T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5d56d50"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[Precision(), Recall()])"
      ],
      "id": "c5d56d50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "414194a3",
        "outputId": "94a6d446-0f7f-4b5d-e3e9-e53b779607ba"
      },
      "source": [
        "history = model.fit(x= train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 5)"
      ],
      "id": "414194a3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "431/800 [===============>..............] - ETA: 27:19 - loss: 0.6246 - precision: 0.6856 - recall: 0.6834"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TDXulPclMxy-",
        "outputId": "e1f42561-205b-43ce-927f-ace7de44eec5"
      },
      "source": [
        "preds_train_1 = model.predict(train_generator, \n",
        "                                   steps=(train_generator.n//20), \n",
        "                                   verbose=1,\n",
        "                                   workers=8)\n",
        "preds_val_1 = model.predict(validation_generator, \n",
        "                                 steps=(validation_generator.n//20),\n",
        "                                 verbose=1,\n",
        "                                 workers=8)"
      ],
      "id": "TDXulPclMxy-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 116s 143ms/step\n",
            "40/40 [==============================] - 6s 121ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lq2hv_aCMx5F",
        "outputId": "23e347bc-93d6-4388-eb1f-cf67f4a97edc"
      },
      "source": [
        "model_metrics = model.evaluate(validation_generator)\n",
        "model_metrics"
      ],
      "id": "Lq2hv_aCMx5F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 10s 247ms/step - loss: 0.4747 - precision_2: 0.7826 - recall_2: 0.8550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47471725940704346, 0.782608687877655, 0.8550000190734863]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "febWJjiZOjYl"
      },
      "source": [
        "perf_df = pd.DataFrame(columns=['model', 'loss', 'precision', 'recall'])\n",
        "perf_df.loc[len(perf_df.index)] = ['model'] + model_metrics"
      ],
      "id": "febWJjiZOjYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfvaUsC1OjbO"
      },
      "source": [
        "perf_df"
      ],
      "id": "sfvaUsC1OjbO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_wkVc1Ojdk"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred,)\n",
        "    \n",
        "    ax= plt.subplot()\n",
        "    # annot=True to annotate cells, fmt='g' to disable scientific notation\n",
        "    sns.heatmap(cm, annot=True, ax=ax, fmt='g', cmap='magma', linewidths=1, linecolor='black')\n",
        "\n",
        "    # labels, title and ticks\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.xaxis.set_ticklabels(['NORMAL', 'PNEUMONIA'])\n",
        "    ax.yaxis.set_ticklabels(['NORMAL', 'PNEUMONIA'])\n",
        "    plt.show();"
      ],
      "id": "jT_wkVc1Ojdk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzZdvoCdOtNd"
      },
      "source": [
        "plot_confusion_matrix(train_generator.labels, np.rint(preds_train_1))"
      ],
      "id": "xzZdvoCdOtNd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYwEDmsUOtPz"
      },
      "source": [
        ""
      ],
      "id": "SYwEDmsUOtPz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0eJIj_VxB49"
      },
      "source": [
        "def visualize_training_results_1(history):\n",
        "    '''\n",
        "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
        "    \n",
        "    Input: keras history object (output from trained model)\n",
        "    '''\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)\n",
        "    fig.suptitle('Model Results')\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    ax1.plot(history.history['recall'])\n",
        "    ax1.plot(history.history['val_recall'])\n",
        "    ax1.set_ylabel('Recall')\n",
        "    ax1.legend(['train', 'test'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['train', 'test'], loc='upper left')\n",
        "    \n",
        "    ax3.plot(history.history['precision'])\n",
        "    ax3.plot(history.history['val_precision'])\n",
        "    ax3.set_ylabel('Precision')\n",
        "    ax3.legend(['train', 'test'], loc='upper left')\n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "    pass"
      ],
      "id": "Q0eJIj_VxB49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEABSmW2kiHw"
      },
      "source": [
        "visualize_training_results_1(history)"
      ],
      "id": "VEABSmW2kiHw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVRb3HwlPF0S"
      },
      "source": [
        "# Model 2 #"
      ],
      "id": "PVRb3HwlPF0S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ly2YLvg1G7r9"
      },
      "source": [
        "model_2 = Sequential()\n",
        "model_2.add(Conv2D(16, kernel_size=(5,5), padding='valid', input_shape = (224,224,3)))\n",
        "model_2.add(MaxPooling2D(3,3))\n",
        "model_2.add(Dense(32, activation='relu'))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(128, activation='relu'))\n",
        "model_2.add(LeakyReLU(alpha=(.3)))\n",
        "model_2.add(Dropout(.20))\n",
        "model_2.add(Dense(256, activation='relu'))\n",
        "model_2.add(Dense(1, activation='sigmoid'))"
      ],
      "id": "Ly2YLvg1G7r9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdOdfAZRG7ud"
      },
      "source": [
        "model_2.summary()"
      ],
      "id": "DdOdfAZRG7ud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2fnTZD5G7xC"
      },
      "source": [
        "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[Precision(), Recall()])"
      ],
      "id": "b2fnTZD5G7xC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AukokUjG7zc"
      },
      "source": [
        "model_2.fit(\n",
        "    x = train_generator,\n",
        "    validation_data = validation_generator,\n",
        "    batch_size = 20,\n",
        "    epochs = 10\n",
        ")"
      ],
      "id": "5AukokUjG7zc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzcEdAZSOXye"
      },
      "source": [
        "preds_train_2 = model_2.predict(train_generator, \n",
        "                                   steps=(train_generator.n//20), \n",
        "                                   verbose=1,\n",
        "                                   workers=8)\n",
        "preds_val_2 = model_2.predict(validation_generator, \n",
        "                                 steps=(validation_generator.n//20),\n",
        "                                 verbose=1,\n",
        "                                 workers=8)"
      ],
      "id": "AzcEdAZSOXye",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9D_H0hjOYkE"
      },
      "source": [
        "model_metrics = model_2.evaluate(validation_generator)\n",
        "model_metrics"
      ],
      "id": "T9D_H0hjOYkE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZcvMmFOYnQ"
      },
      "source": [
        "perf_df = pd.DataFrame(columns=['model', 'loss', 'precision', 'recall'])\n",
        "perf_df.loc[len(perf_df.index)] = ['model'] + model_metrics"
      ],
      "id": "XPZcvMmFOYnQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEgti_-yOYqq"
      },
      "source": [
        "plot_confusion_matrix(train_generator.labels, np.rint(preds_train_2))"
      ],
      "id": "iEgti_-yOYqq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-f-2y1IsCV-"
      },
      "source": [
        "model_2.history.history"
      ],
      "id": "Y-f-2y1IsCV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcse3zpWsCZE"
      },
      "source": [
        "def visualize_training_results(history, iteration):\n",
        "    '''\n",
        "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
        "    \n",
        "    Input: keras history object (output from trained model)\n",
        "    '''\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)\n",
        "    fig.suptitle('Model Results')\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    ax1.plot(history.history['recall_{}'.format(iteration)])\n",
        "    ax1.plot(history.history['val_recall_{}'.format(iteration)])\n",
        "    ax1.set_ylabel('Recall')\n",
        "    ax1.legend(['train', 'test'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['train', 'test'], loc='upper left')\n",
        "    \n",
        "    ax3.plot(history.history['precision_{}'.format(iteration)])\n",
        "    ax3.plot(history.history['val_precision_{}'.format(iteration)])\n",
        "    ax3.set_ylabel('Precision')\n",
        "    ax3.legend(['train', 'test'], loc='upper left')\n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "    pass"
      ],
      "id": "Xcse3zpWsCZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcY2JC2uuk1D"
      },
      "source": [
        "visualize_training_results(model_2.history, 3)"
      ],
      "id": "OcY2JC2uuk1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz-HEZooQg9w"
      },
      "source": [
        "# Model 3 #"
      ],
      "id": "dz-HEZooQg9w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzs5txCruk9f"
      },
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Conv2D(256, kernel_size=(10,10), activation='relu', input_shape = (224,224,3)))\n",
        "model_3.add(MaxPooling2D(7,7))\n",
        "model_3.add(Dense(256, activation='relu'))\n",
        "model_3.add(Conv2D(128, kernel_size=(7,7), activation='relu'))\n",
        "model_3.add(MaxPooling2D(5,5))\n",
        "model_3.add(Dense(64))\n",
        "model_3.add(Flatten())\n",
        "model_3.add(Dropout(.15))\n",
        "model_3.add(Dense(512,activation='relu'))\n",
        "model_3.add(LeakyReLU(alpha=.2))\n",
        "model_3.add(Dropout(.15))\n",
        "model_3.add(Dense(512, activation='relu'))\n",
        "model_3.add(Dense(1, activation='sigmoid'))"
      ],
      "id": "Bzs5txCruk9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrpyaqVlulXA"
      },
      "source": [
        "model_3.summary()"
      ],
      "id": "CrpyaqVlulXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSMRL0kBRpEW"
      },
      "source": [
        "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=[Precision(), Recall()])"
      ],
      "id": "aSMRL0kBRpEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJluWp21SHHG"
      },
      "source": [
        "model_3.fit(x=train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            epochs=10)"
      ],
      "id": "WJluWp21SHHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3riwWNecSHJW"
      },
      "source": [
        ""
      ],
      "id": "3riwWNecSHJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gJxJKV3SHLl"
      },
      "source": [
        ""
      ],
      "id": "5gJxJKV3SHLl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g4pjUz1kz6k"
      },
      "source": [
        "train_path"
      ],
      "id": "0g4pjUz1kz6k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d6af51a"
      },
      "source": [
        "As per the documentation of the dataset it appears that the images in the zenodo dataset are 224x224 pixels. The dataset in the kaggle dataset are 768x768. IN this case we will make the 224x224 size standard accross all images."
      ],
      "id": "6d6af51a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ef38e0b1",
        "outputId": "e56d6fce-6c85-4aee-aff5-ac68c4f96358"
      },
      "source": [
        "train_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_path, \n",
        "                                                                             target_size=(224,224), \n",
        "                                                                             classes=['cancer', 'normal'], \n",
        "                                                                             batch_size=100)\n",
        "validation_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=validation_path,\n",
        "                                                                                  target_size=(224,224), \n",
        "                                                                                  classes=['cancer', 'normal'], \n",
        "                                                                                  batch_size=100)\n",
        "test_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=test_hold_path,\n",
        "                                                                            target_size=(224,224), \n",
        "                                                                            classes=['cancer', 'normal'], \n",
        "                                                                            batch_size=100)"
      ],
      "id": "ef38e0b1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-24f58837ab11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_path, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                              \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                              \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cancer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                              batch_size=100)\n\u001b[1;32m      5\u001b[0m validation_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=validation_path,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b897e323"
      },
      "source": [
        "images_train, label_train = next(train_dataset_batch)\n",
        "images_validation, label_validation = next(validation_dataset_batch)"
      ],
      "id": "b897e323",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bc93207"
      },
      "source": [
        "def plot_image(img):\n",
        "    fig, axes = plt.subplots(1,10, figsize=(10,10))\n",
        "    axes = axes.flatten()\n",
        "    for image, ax in zip(img, axes):\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show"
      ],
      "id": "0bc93207",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1e3891"
      },
      "source": [
        "Based on the order of how we called the classes when defining our batches, [1,0] refers to a normal cell and [0,1] refers to a cancer cell."
      ],
      "id": "ff1e3891"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd828a4e"
      },
      "source": [
        "print(images_train.shape)\n",
        "print(images_validation.shape)"
      ],
      "id": "dd828a4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28e91065"
      },
      "source": [
        "## Build the first CNN ##"
      ],
      "id": "28e91065"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e626a9c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='tanh', padding='same', input_shape=(224,224,3)))\n",
        "model.add(MaxPooling2D(pool_size=(4,4)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dense(2, activation='relu'))"
      ],
      "id": "9e626a9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360d97a6"
      },
      "source": [
        "model.summary()"
      ],
      "id": "360d97a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c9df4d7"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[Precision()])"
      ],
      "id": "6c9df4d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9861aad"
      },
      "source": [
        "model.fit(x=train_dataset_batch,\n",
        "          validation_data = (validation_dataset_batch),\n",
        "          epochs=5)"
      ],
      "id": "b9861aad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28416a4a"
      },
      "source": [
        ""
      ],
      "id": "28416a4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa096284"
      },
      "source": [
        ""
      ],
      "id": "fa096284",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26e95b82"
      },
      "source": [
        ""
      ],
      "id": "26e95b82",
      "execution_count": null,
      "outputs": []
    }
  ]
}